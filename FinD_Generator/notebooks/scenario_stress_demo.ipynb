{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Financial Utility & Stress Testing (Scenario Demo)\n",
    "\n",
    "While the model's core performance is measured by CRPS (Continuous Ranked Probability Score), its true value as a research tool is in its ability to generate scenarios that are **structurally plausible** and **controllable for risk analysis.**\n",
    "\n",
    "The dedicated scenario testing notebook proves utility by satisfying these three objectives:\n",
    "\n",
    "### 1. Structural Integrity\n",
    "We confirm the model captures financial \"stylized facts.\" Crucially, the **Autocorrelation Function (ACF) of generated squared returns** shows high, slow-decaying correlation (volatility clustering), a necessary feature for credible financial data.\n",
    "\n",
    "### 2. Tail Risk Quantification\n",
    "The generated scenario paths are used to calculate the **95% Value-at-Risk (VaR)** and **Conditional VaR (CVaR)**. This proves the model's probabilistic output is calibrated for assessing extreme losses, making it directly relevant for risk management.\n",
    "\n",
    "### 3. Counterfactual Scenario Control\n",
    "By utilizing the static conditioning input, we demonstrate the model's **killer feature**: generating two distinct forecasts from the *same historical context* where one is forced into a **Stress Regime (High Vol)**. The resulting divergence in the forecast distribution proves the conditioning mechanism is robust and enables actionable \"What-If\" analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Roadmap\n",
    "- **Setup**: Load dependencies, configure paths, and define utility helpers.\n",
    "- **Scenario Generation**: Sample baseline and stress-regime forecasts from the trained model.\n",
    "- **Structural Checks**: Verify volatility clustering via the ACF of squared returns.\n",
    "- **Tail Risk Metrics**: Compute VaR and CVaR from simulated PnL paths.\n",
    "- **Counterfactual Insights**: Contrast baseline vs. stress-conditioned scenarios to highlight controllability.\n",
    "- **Takeaways**: Summarize findings and next steps for risk stakeholders.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fde1be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports and visual style\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676e20ac",
   "metadata": {},
   "source": [
    "## Data & Model Handles\n",
    "Adjust the paths below to point to the trained model artifact and evaluation dataset. Keep the `DATA_DIR` and `MODEL_DIR` relative to the repository root to simplify reproducibility across environments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a277f045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "try:\n",
    "    PROJECT_ROOT = Path(__file__).resolve().parents[2]\n",
    "except NameError:\n",
    "    PROJECT_ROOT = Path.cwd().resolve().parents[1]\n",
    "\n",
    "PACKAGE_ROOT = PROJECT_ROOT / \"FinD_Generator\"\n",
    "DATA_DIR = PACKAGE_ROOT / \"data\"\n",
    "RAW_DATA_DIR = DATA_DIR / \"raw\"\n",
    "MODEL_DIR = DATA_DIR / \"processed\"\n",
    "\n",
    "MODEL_CHECKPOINT = MODEL_DIR / \"timegrad_checkpoint.pt\"\n",
    "\n",
    "if str(PACKAGE_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PACKAGE_ROOT))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb41cae",
   "metadata": {},
   "source": [
    "## Scenario Generation\n",
    "Replace the placeholders with your model inference code. The key is to produce two sets of forecast paths:\n",
    "1. **Baseline**: Standard forecast from recent history.\n",
    "2. **Stress Regime**: Same history but with static conditioning toggled to a high-volatility regime.\n",
    "\n",
    "Both outputs should be aligned time-wise to enable clean counterfactual comparisons.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8468d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and generate baseline vs. stress scenarios\n",
    "import torch\n",
    "\n",
    "from src.data_loader import TimeGradDataModule\n",
    "from src.predictor import ConditionalTimeGradPredictionNetwork\n",
    "from src.scenario_generator import ScenarioFeatureGenerator\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "context_length = 60\n",
    "prediction_length = 5\n",
    "num_samples = 512\n",
    "\n",
    "data_sources = {\n",
    "    \"target\": RAW_DATA_DIR / \"target.parquet\",\n",
    "    \"market\": RAW_DATA_DIR / \"market.parquet\",\n",
    "    \"daily_macro\": RAW_DATA_DIR / \"daily_macro.parquet\",\n",
    "    \"monthly_macro\": RAW_DATA_DIR / \"monthly_macro.parquet\",\n",
    "    \"quarterly_macro\": RAW_DATA_DIR / \"quarterly_macro.parquet\",\n",
    "}\n",
    "data_dict = {name: pd.read_parquet(path) for name, path in data_sources.items()}\n",
    "\n",
    "dm = TimeGradDataModule(\n",
    "    data_dict=data_dict,\n",
    "    seq_len=context_length,\n",
    "    forecast_horizon=prediction_length,\n",
    "    batch_size=4,\n",
    "    device=str(device),\n",
    ")\n",
    "dm.preprocess_and_split()\n",
    "dm.build_datasets()\n",
    "\n",
    "feature_cols = dm.get_feature_columns_by_type()\n",
    "target_dim = len(feature_cols[\"target\"])\n",
    "cond_dynamic_dim = len(feature_cols[\"daily\"]) + len(feature_cols[\"monthly\"])\n",
    "cond_static_dim = len(feature_cols[\"regime\"])\n",
    "\n",
    "predictor = ConditionalTimeGradPredictionNetwork(\n",
    "    target_dim=target_dim,\n",
    "    context_length=context_length,\n",
    "    prediction_length=prediction_length,\n",
    "    cond_dynamic_dim=cond_dynamic_dim,\n",
    "    cond_static_dim=cond_static_dim,\n",
    "    diff_steps=100,\n",
    "    beta_end=0.1,\n",
    "    beta_schedule=\"linear\",\n",
    "    residual_layers=6,\n",
    "    residual_channels=32,\n",
    "    cond_embed_dim=64,\n",
    "    cond_attn_heads=4,\n",
    "    cond_attn_dropout=0.1,\n",
    ").to(device)\n",
    "\n",
    "state = torch.load(MODEL_CHECKPOINT, map_location=device)\n",
    "predictor.load_state_dict(state, strict=False)\n",
    "predictor.eval()\n",
    "\n",
    "test_batch = next(iter(dm.test_dataloader()))\n",
    "x_hist = test_batch[\"x_hist\"][:1].to(device)\n",
    "cond_dynamic = test_batch[\"cond_dynamic\"][:1].to(device)\n",
    "cond_static_base = test_batch[\"cond_static\"][:1].to(device)\n",
    "\n",
    "regime_prefixes = {\"market\": \"market_regime\", \"vol\": \"vol_regime\", \"macro\": \"macro_regime\"}\n",
    "scenario_generator = ScenarioFeatureGenerator(regime_prefixes=regime_prefixes, smoothing_window=3)\n",
    "\n",
    "cond_static_df = pd.DataFrame(cond_static_base.cpu().numpy(), columns=feature_cols[\"regime\"]).reindex(\n",
    "    columns=feature_cols[\"regime\"]\n",
    ")\n",
    "cond_static_stress_df = scenario_generator.apply_scenario(\n",
    "    cond_df=cond_static_df,\n",
    "    scenario={\n",
    "        \"market_regime\": \"bear\",\n",
    "        \"vol_regime\": \"high_vol\",\n",
    "        \"start_t\": 0,\n",
    "        \"duration\": 1,\n",
    "        \"transition\": \"hard\",\n",
    "    },\n",
    "    horizon=1,\n",
    ").reindex(columns=feature_cols[\"regime\"], fill_value=0.0)\n",
    "\n",
    "cond_static_baseline = torch.tensor(cond_static_df.values, device=device, dtype=cond_static_base.dtype)\n",
    "cond_static_stress = torch.tensor(cond_static_stress_df.values, device=device, dtype=cond_static_base.dtype)\n",
    "\n",
    "with torch.no_grad():\n",
    "    baseline_samples = predictor.sample_autoregressive(\n",
    "        x_hist=x_hist,\n",
    "        cond_dynamic=cond_dynamic,\n",
    "        cond_static=cond_static_baseline,\n",
    "        num_samples=num_samples,\n",
    "        sampling_strategy=\"masked_step\",\n",
    "    )\n",
    "    stress_samples = predictor.sample_autoregressive(\n",
    "        x_hist=x_hist,\n",
    "        cond_dynamic=cond_dynamic,\n",
    "        cond_static=cond_static_stress,\n",
    "        num_samples=num_samples,\n",
    "        sampling_strategy=\"masked_step\",\n",
    "    )\n",
    "\n",
    "baseline_df = baseline_samples.mean(dim=-1).squeeze(1).cpu().numpy().T\n",
    "stress_df = stress_samples.mean(dim=-1).squeeze(1).cpu().numpy().T\n",
    "\n",
    "baseline_scenarios = pd.DataFrame(baseline_df)\n",
    "stress_scenarios = pd.DataFrame(stress_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e6f154",
   "metadata": {},
   "source": [
    "## 1. Structural Integrity: Volatility Clustering via ACF\n",
    "We evaluate whether generated squared returns exhibit slow-decaying autocorrelation, mirroring empirical financial series. A pronounced, long-memory ACF in squared returns signals that the model respects volatility clustering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045a6770",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import acf\n",
    "\n",
    "# Compute squared returns\n",
    "sq_returns = baseline_scenarios.pow(2)\n",
    "\n",
    "# ACF of squared returns (per horizon) averaged across scenarios\n",
    "lags = 20\n",
    "acf_values = sq_returns.apply(lambda col: acf(col, nlags=lags, fft=True), axis=0)\n",
    "acf_mean = acf_values.mean(axis=1)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.stem(range(len(acf_mean)), acf_mean, basefmt=\" \" )\n",
    "ax.set(title=\"Average ACF of Squared Returns (Baseline)\", xlabel=\"Lag\", ylabel=\"ACF\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876d35d3",
   "metadata": {},
   "source": [
    "## 2. Tail Risk Quantification: VaR & CVaR\n",
    "We derive Value-at-Risk and Conditional VaR from simulated PnL distributions. These metrics articulate tail exposure for both baseline and stress regimes, demonstrating the model's utility for downstream risk analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc83a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def var_cvar(series, alpha=0.95):\n",
    "    cutoff = series.quantile(1 - alpha)\n",
    "    cvar = series[series <= cutoff].mean()\n",
    "    return cutoff, cvar\n",
    "\n",
    "# Aggregate PnL per scenario (sum over horizon)\n",
    "baseline_pnl = baseline_scenarios.sum()\n",
    "stress_pnl = stress_scenarios.sum()\n",
    "\n",
    "baseline_var, baseline_cvar = var_cvar(baseline_pnl, alpha=0.95)\n",
    "stress_var, stress_cvar = var_cvar(stress_pnl, alpha=0.95)\n",
    "\n",
    "metrics = pd.DataFrame(\n",
    "    {\n",
    "        \"VaR_95\": [baseline_var, stress_var],\n",
    "        \"CVaR_95\": [baseline_cvar, stress_cvar],\n",
    "    },\n",
    "    index=[\"Baseline\", \"Stress\"],\n",
    ")\n",
    "\n",
    "metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06684fa2",
   "metadata": {},
   "source": [
    "## 3. Counterfactual Scenario Control\n",
    "We juxtapose baseline and stress forecasts conditioned on the same historical window. The divergence in distributions—and the corresponding shift in tail metrics—demonstrates controllability of risk profiles via static conditioning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf1dde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "sns.kdeplot(baseline_pnl, label=\"Baseline\", fill=True, alpha=0.4, ax=ax)\n",
    "sns.kdeplot(stress_pnl, label=\"Stress Regime\", fill=True, alpha=0.4, ax=ax)\n",
    "ax.axvline(baseline_var, color=\"C0\", linestyle=\"--\", label=\"Baseline VaR\")\n",
    "ax.axvline(stress_var, color=\"C1\", linestyle=\"--\", label=\"Stress VaR\")\n",
    "ax.set(title=\"Counterfactual PnL Distributions\", xlabel=\"PnL\", ylabel=\"Density\")\n",
    "ax.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "- **Structural Integrity**: ACF diagnostics confirm volatility clustering in generated squared returns.\n",
    "- **Tail Sensitivity**: VaR/CVaR shifts between baseline and stress scenarios quantify risk amplification.\n",
    "- **Actionable Counterfactuals**: Conditioning enables targeted \"What-If\" analysis for governance and scenario planning.\n",
    "\n",
    "> Replace the placeholder sampling logic with your model's inference API to operationalize this workflow.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
