{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "fa3db467",
      "metadata": {},
      "source": [
        "# TimeGrad vs Conditional TimeGrad Comparison\n",
        "\n",
        "This notebook trains a lightweight vanilla TimeGrad model and a conditioning-aware TimeGrad variant on the same data, then visualizes their forecasts side by side."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a2eb010",
      "metadata": {},
      "source": [
        "## Setup\n",
        "The imports assume this notebook lives in `notebooks/` under the project root."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "b2f35304",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "import os\n",
        "from pathlib import Path\n",
        "from typing import Tuple\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "try:\n",
        "    project_root = Path(__file__).resolve().parents[1]\n",
        "except NameError:\n",
        "    project_root = Path(os.getcwd()).resolve().parent\n",
        "\n",
        "if str(project_root) not in os.sys.path:\n",
        "    os.sys.path.insert(0, str(project_root))\n",
        "\n",
        "from src.data_loader import TimeGradDataModule\n",
        "from src.models.timegrad_core.timegrad_base import TimeGradBase\n",
        "from src.predictor import ConditionalTimeGradPredictionNetwork\n",
        "from src.training import ConditionalTimeGradTrainingNetwork\n",
        "\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b07214af",
      "metadata": {},
      "source": [
        "## Load & preprocess data\n",
        "This matches the preprocessing flow used in the conditional demo notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "1a8d02be",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'target': (7052, 6), 'market': (7052, 6), 'daily_macro': (7305, 3), 'monthly_macro': (336, 5), 'quarterly_macro': (112, 6)}\n",
            "ðŸ”§ [Init] Aligning and preparing macroeconomic data...\n",
            "align_and_handle_missing_values: daily_df columns before processing: ['Date', 'yield_curve', 'vix']\n",
            "align_and_handle_missing_values: daily_aligned columns after processing: ['yield_curve_daily', 'vix_daily']\n",
            "âœ… [Init] Macro data alignment complete.\n",
            "\n",
            "build_raw_blocks: daily_macro_indexed columns before calling process_daily_macro_raw: ['yield_curve_daily', 'vix_daily']\n",
            "ðŸ”„ [build_raw_blocks] Processing raw data blocks...\n",
            "âœ… Target wavelet denoising complete.\n",
            "âœ… Market log-returns computed.\n",
            "âœ… Daily macro block processed.\n",
            "âœ… Monthly macro transformations complete.\n",
            "âœ… Quarterly macro transformations complete.\n",
            "ðŸ—ï¸ [build_raw_blocks] All raw data blocks prepared.\n",
            "\n",
            "ðŸ”„ [preprocess_raw_merge] Merging all blocks into unified DataFrame...\n",
            "âœ… All blocks merged successfully.\n",
            "ðŸ”„ Adding calendar and regime features...\n",
            "âœ… Calendar features and regime labels added.\n",
            "\n",
            "ðŸ“Š [Split] Dataset split into Train (5112), Val (1095), Test (1096).\n",
            "\n",
            "ðŸ”„ [Scaling/PCA] Fitting target scaler and PCA...\n",
            "âœ… Done.\n",
            "ðŸ”„ [Scaling/PCA] Fitting market scaler and PCA...\n",
            "âœ… Done.\n",
            "ðŸ”„ [Scaling/PCA] Fitting daily macro scaler...\n",
            "âœ… Done.\n",
            "ðŸ”„ [Scaling/PCA] Fitting monthly macro scaler and PCA...\n",
            "âœ… Done.\n",
            "ðŸ”„ [Scaling/PCA] Fitting quarterly macro scaler and PCA...\n",
            "âœ… Done.\n",
            "[fit_transform_train] train_df rows: 5112\n",
            "[fit_transform_train] target cols: ['open_den', 'high_den', 'low_den', 'close_den'] -> train shape (5112, 4)\n",
            "[fit_transform_train] market cols: ['open_ret', 'high_ret', 'low_ret', 'close_ret'] -> train shape (5112, 4)\n",
            "[fit_transform_train] example volume_raw len: 5112\n",
            "ðŸ”„ [Transform] Applying fitted scalers and PCA to all splits...\n",
            "[target PCA] X_t (5112, 4), scaled (5112, 4), pca (5112, 1), index 5112\n",
            "[target PCA] t_pca_df (5112, 1)\n",
            "[fit_volume_scaler] fitting StandardScaler on volume_raw (train only)\n",
            "[volume debug] len(df)=5112, vol_vals.shape=(10224, 1), vol_scaled.shape=(10224, 1)\n",
            "[volume_scaled] df len=5112, vol_scaled shape=(5112, 1)\n",
            "[transform_df end] final df shape: (5112, 46)\n",
            "âœ… [Transform] All datasets transformed successfully.\n",
            "\n",
            "[target PCA] X_t (1095, 4), scaled (1095, 4), pca (1095, 1), index 1095\n",
            "[target PCA] t_pca_df (1095, 1)\n",
            "[volume debug] len(df)=1095, vol_vals.shape=(2190, 1), vol_scaled.shape=(2190, 1)\n",
            "[volume_scaled] df len=1095, vol_scaled shape=(1095, 1)\n",
            "[transform_df end] final df shape: (1095, 46)\n",
            "âœ… [Transform] All datasets transformed successfully.\n",
            "\n",
            "[target PCA] X_t (1096, 4), scaled (1096, 4), pca (1096, 1), index 1096\n",
            "[target PCA] t_pca_df (1096, 1)\n",
            "[volume debug] len(df)=1096, vol_vals.shape=(2192, 1), vol_scaled.shape=(2192, 1)\n",
            "[volume_scaled] df len=1096, vol_scaled shape=(1096, 1)\n",
            "[transform_df end] final df shape: (1096, 46)\n",
            "âœ… [Transform] All datasets transformed successfully.\n",
            "\n",
            "[ _transform_all_splits ] train_transformed rows: 5112 cols: 46\n",
            "[ _transform_all_splits ] val_transformed rows:   1095 cols: 46\n",
            "[ _transform_all_splits ] test_transformed rows:  1096 cols: 46\n",
            "âœ… Datasets built. Train: 5025, Val: 1008, Test: 1009 samples.\n",
            "Feature columns by type:\n",
            "- target: ['target_pca_1']\n",
            "- daily: ['market_pca_1', 'market_pca_2', 'market_pca_3', 'daily_vix_daily_scaled', 'daily_yield_curve_daily_scaled', 'volume_scaled', 'day_of_week', 'month', 'quarter', 'year', 'is_month_end', 'is_quarter_end', 'month_sin', 'month_cos', 'dow_sin', 'dow_cos', 'quarter_sin', 'quarter_cos']\n",
            "- monthly: ['monthly_pca_1', 'monthly_pca_2', 'monthly_pca_3', 'monthly_pca_4']\n",
            "- regime: ['market_regime_bear', 'market_regime_bull', 'market_regime_sideways', 'vol_regime_high_vol', 'vol_regime_normal_vol', 'macro_regime_normal']\n"
          ]
        }
      ],
      "source": [
        "data_base = project_root / 'data'\n",
        "raw_base = data_base / 'raw'\n",
        "raw_paths = {\n",
        "    'target': raw_base / 'target.parquet',\n",
        "    'market': raw_base / 'market.parquet',\n",
        "    'daily_macro': raw_base / 'daily_macro.parquet',\n",
        "    'monthly_macro': raw_base / 'monthly_macro.parquet',\n",
        "    'quarterly_macro': raw_base / 'quarterly_macro.parquet',\n",
        "}\n",
        "\n",
        "data_dict = {name: pd.read_parquet(path) for name, path in raw_paths.items()}\n",
        "print({k: v.shape for k, v in data_dict.items()})\n",
        "\n",
        "context_length = 64\n",
        "prediction_length = 24\n",
        "batch_size = 4\n",
        "\n",
        "dm = TimeGradDataModule(\n",
        "    data_dict=data_dict,\n",
        "    seq_len=context_length,\n",
        "    forecast_horizon=prediction_length,\n",
        "    batch_size=batch_size,\n",
        "    device=str(device),\n",
        ")\n",
        "\n",
        "dm.preprocess_and_split()\n",
        "dm.build_datasets()\n",
        "\n",
        "feature_cols = dm.get_feature_columns_by_type()\n",
        "print('Feature columns by type:')\n",
        "for k, v in feature_cols.items():\n",
        "    print(f'- {k}: {v}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d9c11651",
      "metadata": {},
      "source": [
        "## Minimal vanilla TimeGrad wrapper\n",
        "The helper below mirrors the conditional wrapper's normalization but omits conditioning. It allows quick training and sampling for the baseline model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "059eb3cd",
      "metadata": {},
      "outputs": [],
      "source": [
        "class VanillaTimeGradWrapper(nn.Module):\n",
        "    def __init__(self, target_dim: int, prediction_length: int, scale_eps: float = 1e-5):\n",
        "        super().__init__()\n",
        "        self.prediction_length = prediction_length\n",
        "        self.scale_eps = scale_eps\n",
        "        self.model = TimeGradBase(target_dim=target_dim, prediction_length=prediction_length)\n",
        "\n",
        "    def _normalize(self, x_hist: torch.Tensor, x_future: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "        combined = torch.cat([x_hist, x_future], dim=1)\n",
        "        loc = combined.mean(dim=1, keepdim=True)\n",
        "        scale = combined.std(dim=1, keepdim=True).clamp_min(self.scale_eps)\n",
        "        return loc, scale, (x_future - loc) / scale\n",
        "\n",
        "    def forward(self, x_hist: torch.Tensor, x_future: torch.Tensor) -> torch.Tensor:\n",
        "        loc, scale, x_future_norm = self._normalize(x_hist, x_future)\n",
        "        loss = self.model(x_future_norm)\n",
        "        return loss\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def sample(self, x_hist: torch.Tensor, num_samples: int = 100) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        loc = x_hist.mean(dim=1, keepdim=True)\n",
        "        scale = x_hist.std(dim=1, keepdim=True).clamp_min(self.scale_eps)\n",
        "        samples = self.model.diffusion.sample(\n",
        "            batch_size=x_hist.shape[0], horizon=self.prediction_length\n",
        "        )\n",
        "        samples = samples.transpose(1, 2) * scale + loc\n",
        "        return samples, loc.squeeze(1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e3b7092",
      "metadata": {},
      "source": [
        "## Train both models (small demo epochs)\n",
        "Training loops keep epochs small for a quick comparison; increase them for higher fidelity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "f08c476f",
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "'cond_dynamic'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      1\u001b[39m vanilla = VanillaTimeGradWrapper(target_dim=\u001b[38;5;28mlen\u001b[39m(feature_cols[\u001b[33m'\u001b[39m\u001b[33mtarget\u001b[39m\u001b[33m'\u001b[39m]), prediction_length=prediction_length).to(device)\n\u001b[32m      2\u001b[39m conditional_train = ConditionalTimeGradTrainingNetwork(\n\u001b[32m      3\u001b[39m     target_dim=\u001b[38;5;28mlen\u001b[39m(feature_cols[\u001b[33m'\u001b[39m\u001b[33mtarget\u001b[39m\u001b[33m'\u001b[39m]),\n\u001b[32m      4\u001b[39m     context_length=context_length,\n\u001b[32m      5\u001b[39m     prediction_length=prediction_length,\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     cond_dynamic_dim=\u001b[38;5;28mlen\u001b[39m(\u001b[43mfeature_cols\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcond_dynamic\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m),\n\u001b[32m      7\u001b[39m     cond_static_dim=\u001b[38;5;28mlen\u001b[39m(feature_cols[\u001b[33m'\u001b[39m\u001b[33mcond_static\u001b[39m\u001b[33m'\u001b[39m]),\n\u001b[32m      8\u001b[39m ).to(device)\n\u001b[32m     10\u001b[39m vanilla_opt = torch.optim.Adam(vanilla.parameters(), lr=\u001b[32m1e-3\u001b[39m)\n\u001b[32m     11\u001b[39m conditional_opt = torch.optim.Adam(conditional_train.parameters(), lr=\u001b[32m1e-3\u001b[39m)\n",
            "\u001b[31mKeyError\u001b[39m: 'cond_dynamic'"
          ]
        }
      ],
      "source": [
        "vanilla = VanillaTimeGradWrapper(target_dim=len(feature_cols['target']), prediction_length=prediction_length).to(device)\n",
        "conditional_train = ConditionalTimeGradTrainingNetwork(\n",
        "    target_dim=len(feature_cols['target']),\n",
        "    context_length=context_length,\n",
        "    prediction_length=prediction_length,\n",
        "    cond_dynamic_dim=len(feature_cols['cond_dynamic']),\n",
        "    cond_static_dim=len(feature_cols['cond_static']),\n",
        ").to(device)\n",
        "\n",
        "vanilla_opt = torch.optim.Adam(vanilla.parameters(), lr=1e-3)\n",
        "conditional_opt = torch.optim.Adam(conditional_train.parameters(), lr=1e-3)\n",
        "\n",
        "num_epochs = 3\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    vanilla.train(); conditional_train.train()\n",
        "    for batch in DataLoader(dm.train_set, batch_size=batch_size, shuffle=True):\n",
        "        x_hist = batch['x_hist'].to(device)\n",
        "        x_future = batch['x_future'].to(device)\n",
        "        cond_dynamic = batch['cond_dynamic'].to(device)\n",
        "        cond_static = batch['cond_static'].to(device)\n",
        "\n",
        "        # Vanilla\n",
        "        vanilla_opt.zero_grad()\n",
        "        vanilla_loss = vanilla(x_hist, x_future)\n",
        "        vanilla_loss.backward()\n",
        "        vanilla_opt.step()\n",
        "\n",
        "        # Conditional\n",
        "        conditional_opt.zero_grad()\n",
        "        cond_loss = conditional_train(x_hist, x_future, cond_dynamic, cond_static)\n",
        "        cond_loss.backward()\n",
        "        conditional_opt.step()\n",
        "\n",
        "    print(f'Epoch {epoch+1}: vanilla_loss={vanilla_loss.item():.4f}, conditional_loss={cond_loss.item():.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "706553f8",
      "metadata": {},
      "source": [
        "## Build conditional predictor for sampling\n",
        "The predictor reuses the trained conditional backbone and supports masked or full-horizon sampling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf48ab6d",
      "metadata": {},
      "outputs": [],
      "source": [
        "conditional_pred = ConditionalTimeGradPredictionNetwork(\n",
        "    target_dim=len(feature_cols['target']),\n",
        "    context_length=context_length,\n",
        "    prediction_length=prediction_length,\n",
        "    cond_dynamic_dim=len(feature_cols['cond_dynamic']),\n",
        "    cond_static_dim=len(feature_cols['cond_static']),\n",
        ").to(device)\n",
        "conditional_pred.model.load_state_dict(conditional_train.model.state_dict())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e3ad741",
      "metadata": {},
      "source": [
        "## Compare forecasts on a held-out batch\n",
        "We draw a batch from the test split, generate multiple samples from each model, and plot their mean forecasts against the ground truth target window."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9cf57ede",
      "metadata": {},
      "outputs": [],
      "source": [
        "test_batch = next(iter(dm.test_dataloader()))\n",
        "x_hist = test_batch['x_hist'].to(device)\n",
        "x_future = test_batch['x_future'].to(device)\n",
        "cond_dynamic = test_batch['cond_dynamic'].to(device)\n",
        "cond_static = test_batch['cond_static'].to(device)\n",
        "\n",
        "# Vanilla samples\n",
        "vanilla.eval()\n",
        "vanilla_samples, _ = vanilla.sample(x_hist, num_samples=200)\n",
        "vanilla_mean = vanilla_samples.mean(dim=0).detach().cpu().numpy()[0]\n",
        "\n",
        "# Conditional samples\n",
        "conditional_pred.eval()\n",
        "cond_samples = conditional_pred.sample_autoregressive(\n",
        "    x_hist=x_hist,\n",
        "    cond_dynamic=cond_dynamic,\n",
        "    cond_static=cond_static,\n",
        "    num_samples=200,\n",
        "    sampling_strategy='masked_step',\n",
        ")\n",
        "cond_mean = cond_samples.mean(dim=0).detach().cpu().numpy()[0]\n",
        "\n",
        "truth = x_future[0].cpu().numpy()\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(truth[:, 0], label='Ground truth', linewidth=2)\n",
        "plt.plot(vanilla_mean[:, 0], label='Vanilla TimeGrad mean')\n",
        "plt.plot(cond_mean[:, 0], label='Conditional TimeGrad mean')\n",
        "plt.xlabel('Horizon step')\n",
        "plt.ylabel('Target value')\n",
        "plt.title('Forecast comparison')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
